from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi import Request
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware
import hashlib
import hmac
import json
import urllib.parse
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from database import SQLDatabase as db
from config.config import load_config
import logging
from utils import setup_logger
import time

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
from services.search import SearchService
from services.search.arxiv_service import ArxivSearcher
from services.search.semantic_scholar_service import SemanticScholarSearcher
from services.search.ieee_service import IEEESearcher
from services.search.ncbi_service import NCBISearcher
from services.utils.paper import Paper
from nlu import NLUPipeline, Intent  # –ù–æ–≤—ã–π NLU
from nlu.classifiers import LLMIntentClassifier, LLMEntityExtractor
from services.utils.search_utils import SearchUtils
from services.llm import ChatService, PaperService
import asyncio

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_logger(name="api_logger", log_file="logs/api.log", level=logging.DEBUG)

app = FastAPI(title="Scientific Assistant API", version="1.0.0")
start_time = time.time()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
_nlu_pipeline: NLUPipeline = None
_chat_service: ChatService = None
_paper_service: PaperService = None
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
config = load_config()

# CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Mini App
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://t.me", "https://web.telegram.org"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏ —à–∞–±–ª–æ–Ω—ã
app.mount("/static", StaticFiles(directory="webapp/static"), name="static")
templates = Jinja2Templates(directory="webapp/templates")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class PaperTags(BaseModel):
    new_tags: str

class UserLibrary(BaseModel):
    papers: List[Dict[str, Any]]
    total_count: int
    user_id: int

class TelegramInitData(BaseModel):
    user_id: int
    username: Optional[str] = None
    first_name: Optional[str] = None
    last_name: Optional[str] = None

class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = {}
    limit: int = 10
    source: Optional[str] = None  # arxiv, ieee, ncbi, semantic_scholar

class RecommendationRequest(BaseModel):
    paper_ids: List[str]
    limit: int = 10

class ChatRequest(BaseModel):
    message: str
    context: Optional[List[Dict[str, Any]]] = []


class ChatResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –æ—Ç —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    intent: str
    confidence: float
    entities: List[Dict[str, Any]]
    response_text: str
    action: Optional[str] = None
    data: Dict[str, Any] = {}
    needs_cloud_llm: bool = False


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ API."""
    global _nlu_pipeline, _chat_service, _paper_service
    
    from config.constants import OLLAMA_BASE_URL
    
    _nlu_pipeline = NLUPipeline(
        ollama_url=OLLAMA_BASE_URL,
        db_path="db/scientific_assistant.db"
    )
    _chat_service = ChatService(ollama_url=OLLAMA_BASE_URL)
    _paper_service = PaperService()
    
    await _chat_service.initialize()
    logger.info("API chat services initialized")


@app.on_event("shutdown")
async def shutdown_event():
    """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ API."""
    global _nlu_pipeline, _chat_service, _paper_service
    
    if _nlu_pipeline:
        await _nlu_pipeline.close()
    if _chat_service:
        await _chat_service.close()
    if _paper_service:
        await _paper_service.close()
    
    logger.info("API chat services closed")

def validate_telegram_init_data(init_data: str) -> Optional[Dict[str, Any]]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö initData –æ—Ç Telegram
    
    Args:
        init_data: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç Telegram
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        # –ü–∞—Ä—Å–∏–º query string
        parsed_data = urllib.parse.parse_qs(init_data)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º hash –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        received_hash = parsed_data.get('hash', [None])[0]
        if not received_hash:
            logger.warning("–ù–µ—Ç hash –≤ initData")
            return None
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–ø–∏—Å–∏
        auth_data = []
        for key in sorted(parsed_data.keys()):
            if key != 'hash':
                auth_data.append(f"{key}={parsed_data[key][0]}")
        
        auth_string = '\n'.join(auth_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–ø–∏—Å—å
        secret_key = hmac.new(
            "WebAppData".encode(), 
            config.BOT_TOKEN.encode(), 
            hashlib.sha256
        ).digest()
        
        calculated_hash = hmac.new(
            secret_key,
            auth_string.encode(),
            hashlib.sha256
        ).hexdigest()
        
        if not hmac.compare_digest(received_hash, calculated_hash):
            logger.warning("–ù–µ–≤–µ—Ä–Ω–∞—è –ø–æ–¥–ø–∏—Å—å initData")
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_data = parsed_data.get('user', [None])[0]
        if user_data:
            user_info = json.loads(user_data)
            return {
                'user_id': user_info.get('id'),
                'username': user_info.get('username'),
                'first_name': user_info.get('first_name'),
                'last_name': user_info.get('last_name')
            }
        
        return None
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ initData: {e}")
        return None

def get_current_user(request: Request) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ initData"""
    init_data = request.headers.get('X-Telegram-Init-Data')
    if not init_data:
        init_data = request.query_params.get('initData')
    
    if not init_data:
        raise HTTPException(status_code=401, detail="–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    user_data = validate_telegram_init_data(init_data)
    if not user_data:
        raise HTTPException(status_code=401, detail="–ù–µ–≤–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    return user_data

@app.get("/", response_class=HTMLResponse)
async def mini_app_root(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Mini App"""
    return templates.TemplateResponse("library.html", {"request": request})

start_time = time.time()


@app.get("/health", status_code=status.HTTP_200_OK)
async def health_check():
    """
    Health check endpoint –¥–ª—è Docker healthcheck
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞ –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
    """
    uptime = time.time() - start_time
    
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={
            "status": "healthy",
            "uptime_seconds": round(uptime, 2),
            "service": "api"
        }
    )

@app.get("/api/v1/user/info")
async def get_user_info(current_user: Dict[str, Any] = Depends(get_current_user)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ"""
    return {
        "user_id": current_user["user_id"],
        "username": current_user.get("username"),
        "first_name": current_user.get("first_name"),
        "last_name": current_user.get("last_name")
    }

@app.get("/api/v1/library", response_model=UserLibrary)
async def get_user_library(
    page: int = 1,
    per_page: int = 10,
    search: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ –ø–æ–∏—Å–∫–æ–º
    
    Args:
        page: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∏–Ω–∞—è —Å 1)
        per_page: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        search: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        current_user: –î–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        UserLibrary: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    try:
        user_id = current_user["user_id"]
        logger.info(f"–ó–∞–ø—Ä–æ—Å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}, –ø–æ–∏—Å–∫: '{search}'")
        
        if search:
            all_papers = await db.search_in_library(user_id, search)
        else:
            all_papers = await db.get_user_library(user_id)
        
        total_count = len(all_papers)
        
        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
        start_index = (page - 1) * per_page
        end_index = start_index + per_page
        papers = all_papers[start_index:end_index]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        formatted_papers = []
        for paper in papers:
            formatted_papers.append({
                "id": paper.get("id"),
                "title": paper.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                "authors": paper.get("authors", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∞–≤—Ç–æ—Ä—ã"),
                "abstract": paper.get("abstract", "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"),
                "url": paper.get("url", ""),
                "publication_date": paper.get("publication_date", ""),
                "saved_at": paper.get("saved_at", ""),
                "tags": paper.get("tags", []),
                "external_id": paper.get("external_id", ""),
                "source": paper.get("source", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"),
                
            })
        
        return UserLibrary(
            papers=formatted_papers,
            total_count=total_count,
            user_id=user_id
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")

@app.delete("/api/v1/library/{paper_id}")
async def delete_paper_from_library(
    paper_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"""
    try:
        user_id = current_user["user_id"]
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –ø—ã—Ç–∞–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ç—å—é {paper_id}")
        
        success = await db.delete_paper(user_id, paper_id)
        
        if success:
            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–∏–ª —Å—Ç–∞—Ç—å—é {paper_id}")
            return {"message": "–°—Ç–∞—Ç—å—è —É–¥–∞–ª–µ–Ω–∞", "success": True}
        else:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é {paper_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            raise HTTPException(status_code=404, detail="–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏")

@app.get("/api/v1/stats")
async def get_library_stats(current_user: Dict[str, Any] = Depends(get_current_user)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        user_id = current_user["user_id"]
        papers = await db.get_user_library(user_id)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories_count = {}
        for paper in papers:
            if paper.get("categories"):
                for category in paper["categories"]:
                    category = category.strip()
                    categories_count[category] = categories_count.get(category, 0) + 1
        
        return {
            "total_papers": len(papers),
            "categories_distribution": categories_count,
            "user_id": user_id
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")

@app.post("/api/v1/library/{paper_id}/tags")
async def edit_paper_tags(
    paper_id: str,
    tags_data: PaperTags,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç—å–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ"""
    try:
        user_id = current_user["user_id"]
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –ø—ã—Ç–∞–µ—Ç—Å—è –∏–∑–º–µ–Ω–∏—Ç—å —Ç–µ–≥–∏ —Å—Ç–∞—Ç—å–∏ {paper_id} –Ω–∞ '{tags_data.new_tags}'")
        paper_id = paper_id.replace('BACKSLASH', '/')
        
        success = await db.edit_paper_tags(user_id, paper_id, tags_data.new_tags)
        
        if success:
            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} —É—Å–ø–µ—à–Ω–æ –∏–∑–º–µ–Ω–∏–ª —Ç–µ–≥–∏ —Å—Ç–∞—Ç—å–∏ {paper_id}")
            return {"message": "–¢–µ–≥–∏ —Å—Ç–∞—Ç—å–∏ –∏–∑–º–µ–Ω–µ–Ω—ã", "success": True}
        else:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é {paper_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            raise HTTPException(status_code=404, detail="–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç—å–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç—å–∏")

# –ù–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

@app.post("/api/v1/search")
async def search_papers(
    search_request: SearchRequest,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    """
    try:
        user_id = current_user["user_id"]
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –∏—â–µ—Ç: '{search_request.query}'")
        
        results = []
        
        if search_request.source == "arxiv":
            async with ArxivSearcher() as searcher:
                papers = await searcher.search_papers(
                    search_request.query, 
                    limit=search_request.limit,
                    filters=search_request.filters
                )
                results = papers
        elif search_request.source == "ieee":
            async with IEEESearcher() as searcher:
                papers = await searcher.search_papers(
                    search_request.query,
                    limit=search_request.limit,
                    filters=search_request.filters
                )
                results = papers
        elif search_request.source == "ncbi":
            async with NCBISearcher() as searcher:
                papers = await searcher.search_papers(
                    search_request.query,
                    limit=search_request.limit,
                    filters=search_request.filters
                )
                results = papers
        elif search_request.source == "semantic_scholar":
            async with SemanticScholarSearcher() as searcher:
                papers = await searcher.search_papers(
                    search_request.query,
                    limit=search_request.limit,
                    filters=search_request.filters
                )
                results = papers
        else:
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            async with SearchService() as search_service:
                search_results = await search_service.search_papers(
                    search_request.query,
                    limit=search_request.limit,
                    filters=search_request.filters
                )
                results = search_service.aggregate_results(search_results, search_request.query)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
        saved_urls = await SearchUtils._get_user_saved_urls(user_id)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        formatted_results = []
        for paper in results:
            paper_dict = paper.to_dict() if hasattr(paper, 'to_dict') else paper.__dict__
            paper_dict['is_saved'] = paper_dict.get('url', '') in saved_urls
            formatted_results.append(paper_dict)
        
        return {
            "papers": formatted_results,
            "total_count": len(formatted_results),
            "query": search_request.query,
            "source": search_request.source or "all"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞")

@app.post("/api/v1/recommendations")
async def get_recommendations(
    recommendation_request: RecommendationRequest,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    """
    try:
        user_id = current_user["user_id"]
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
        
        if recommendation_request.paper_ids:
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö ID
            papers = [Paper(external_id=paper_id) for paper_id in recommendation_request.paper_ids]
            async with SemanticScholarSearcher() as searcher:
                recommendations = await searcher.get_recommendations_for_multiple_papers(
                    papers, 
                    recommendation_request.limit
                )
        else:
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_papers = await db.get_user_library(user_id)
            if not user_papers:
                return {"papers": [], "total_count": 0, "message": "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞"}
            
            papers = [Paper(**paper) for paper in user_papers[:10]]  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å—Ç–∞—Ç–µ–π
            async with SemanticScholarSearcher() as searcher:
                recommendations = await searcher.get_recommendations_for_multiple_papers(
                    papers, 
                    recommendation_request.limit
                )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Å—Ç–∞—Ç—å–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
        saved_urls = await SearchUtils._get_user_saved_urls(user_id)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        formatted_results = []
        for paper in recommendations:
            paper_dict = paper.to_dict() if hasattr(paper, 'to_dict') else paper.__dict__
            paper_dict['is_saved'] = paper_dict.get('url', '') in saved_urls
            formatted_results.append(paper_dict)
        
        return {
            "papers": formatted_results,
            "total_count": len(formatted_results)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

@app.post("/api/v1/chat", response_model=ChatResponse)
async def chat_with_assistant(
    chat_request: ChatRequest,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    –ß–∞—Ç —Å AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç NLU Pipeline –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π.
    """
    global _nlu_pipeline, _chat_service, _paper_service
    
    try:
        user_id = current_user["user_id"]
        message = chat_request.message.strip()
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –æ—Ç–ø—Ä–∞–≤–∏–ª —Å–æ–æ–±—â–µ–Ω–∏–µ: '{message}'")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NLU Pipeline
        nlu_result = await _nlu_pipeline.process(user_id=user_id, message=message)
        
        intent = nlu_result.intent.intent
        entities = [
            {
                "type": e.type.value,
                "value": e.value,
                "confidence": e.confidence,
                "normalized": e.normalized_value
            }
            for e in nlu_result.entities.entities
        ]
        
        response_text = ""
        action = None
        data = {}
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –∏–Ω—Ç–µ–Ω—Ç–∞–º
        if intent == Intent.SEARCH:
            # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
            query = nlu_result.query_params.get("query", message)
            filters = {}
            
            if nlu_result.query_params.get("year"):
                filters["year"] = nlu_result.query_params["year"]
            if nlu_result.query_params.get("author"):
                filters["author"] = nlu_result.query_params["author"]
            
            source = nlu_result.query_params.get("source")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            try:
                search_results = []
                if source == "arxiv":
                    async with ArxivSearcher() as searcher:
                        search_results = await searcher.search_papers(query, limit=10, filters=filters)
                elif source == "ieee":
                    async with IEEESearcher() as searcher:
                        search_results = await searcher.search_papers(query, limit=10, filters=filters)
                elif source == "ncbi":
                    async with NCBISearcher() as searcher:
                        search_results = await searcher.search_papers(query, limit=10, filters=filters)
                elif source == "semantic_scholar":
                    async with SemanticScholarSearcher() as searcher:
                        search_results = await searcher.search_papers(query, limit=10, filters=filters)
                else:
                    async with SearchService() as search_service:
                        search_results = await search_service.search_papers(query, limit=10, filters=filters)
                        search_results = search_service.aggregate_results(search_results, query)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                formatted_results = []
                for paper in search_results:
                    paper_dict = paper.to_dict() if hasattr(paper, 'to_dict') else paper.__dict__
                    formatted_results.append(paper_dict)
                
                data["papers"] = formatted_results
                data["query"] = query
                action = "show_search_results"
                
                if formatted_results:
                    response_text = f"üîç –ù–∞–π–¥–µ–Ω–æ {len(formatted_results)} —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª"
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
                    await _nlu_pipeline.update_context(
                        user_id=user_id,
                        message=message,
                        result=nlu_result,
                        bot_response=response_text,
                        search_results=formatted_results[:10]
                    )
                else:
                    response_text = f"üòî –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                response_text = "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
        elif intent == Intent.LIST_LIBRARY:
            # –ü–æ–∫–∞–∑–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É
            papers = await db.get_user_library(user_id)
            formatted_papers = [
                {
                    "id": p.get("id"),
                    "title": p.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                    "authors": p.get("authors", ""),
                    "url": p.get("url", ""),
                }
                for p in papers[:20]
            ]
            data["papers"] = formatted_papers
            action = "show_library"
            response_text = f"üìö –í –≤–∞—à–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ {len(papers)} —Å—Ç–∞—Ç–µ–π"
        
        elif intent == Intent.GET_SUMMARY:
            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏
            article = nlu_result.query_params.get("article")
            if article:
                try:
                    summary = await _paper_service.summarize(article)
                    data["summary"] = summary
                    data["article"] = article
                    action = "show_summary"
                    response_text = summary
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
                    response_text = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–∞–º–º–∞—Ä–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            else:
                response_text = "ü§î –£–∫–∞–∂–∏—Ç–µ —Å—Ç–∞—Ç—å—é –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –ù–∞–ø—Ä–∏–º–µ—Ä: ¬´–ö—Ä–∞—Ç–∫–æ –æ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–µ¬ª"
        
        elif intent == Intent.EXPLAIN:
            # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–ª–∏ —Å—Ç–∞—Ç—å–∏
            article = nlu_result.query_params.get("article")
            if article:
                try:
                    explanation = await _paper_service.explain(article)
                    data["explanation"] = explanation
                    data["article"] = article
                    action = "show_explanation"
                    response_text = explanation
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {e}")
                    response_text = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            else:
                # –û–±—â–µ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —á–∞—Ç
                context = await _nlu_pipeline.context_manager.get_context(user_id)
                response_text = await _chat_service.chat(message, context=context)
                action = "chat_response"
        
        elif intent == Intent.COMPARE:
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π
            articles = nlu_result.query_params.get("articles", [])
            if len(articles) >= 2:
                try:
                    comparison = await _paper_service.compare(articles[:5])
                    data["comparison"] = comparison
                    data["articles"] = articles
                    action = "show_comparison"
                    response_text = comparison
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
                    response_text = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å —Å—Ç–∞—Ç—å–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            else:
                response_text = "ü§î –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Å—Ç–∞—Ç—å–∏. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–∏—Å–∫."
        
        elif intent == Intent.SAVE_ARTICLE:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            article = nlu_result.query_params.get("article")
            if article:
                success = await db.save_paper(user_id, article)
                if success:
                    response_text = f"‚úÖ –°—Ç–∞—Ç—å—è ¬´{article.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...¬ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É"
                    action = "article_saved"
                else:
                    response_text = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç—å—é"
            else:
                response_text = "ü§î –£–∫–∞–∂–∏—Ç–µ —Å—Ç–∞—Ç—å—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä: ¬´–°–æ—Ö—Ä–∞–Ω–∏ –ø–µ—Ä–≤—É—é —Å—Ç–∞—Ç—å—é¬ª"
        
        elif intent == Intent.DELETE_ARTICLE:
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            article = nlu_result.query_params.get("article")
            if article and article.get("id"):
                success = await db.delete_paper(user_id, article["id"])
                if success:
                    response_text = "üóëÔ∏è –°—Ç–∞—Ç—å—è —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
                    action = "article_deleted"
                else:
                    response_text = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ç—å—é"
            else:
                response_text = "ü§î –£–∫–∞–∂–∏—Ç–µ —Å—Ç–∞—Ç—å—é –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è"
        
        elif intent == Intent.HELP:
            response_text = """ü§ñ **–Ø ‚Äî AI Scientific Assistant (AISA)**

–í–æ—Ç —á—Ç–æ —è —É–º–µ—é:

üîç **–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π:**
‚Ä¢ ¬´–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ machine learning¬ª
‚Ä¢ ¬´–°—Ç–∞—Ç—å–∏ –ø–æ NLP –∑–∞ 2024 –≥–æ–¥¬ª
‚Ä¢ ¬´–ü–æ–∏—Å–∫ –≤ arxiv: transformers¬ª

üìö **–†–∞–±–æ—Ç–∞ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π:**
‚Ä¢ ¬´–ü–æ–∫–∞–∂–∏ –º–æ—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É¬ª
‚Ä¢ ¬´–°–æ—Ö—Ä–∞–Ω–∏ –ø–µ—Ä–≤—É—é —Å—Ç–∞—Ç—å—é¬ª

üìù **–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π:**
‚Ä¢ ¬´–ö—Ä–∞—Ç–∫–æ –æ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–µ¬ª
‚Ä¢ ¬´–û–±—ä—è—Å–Ω–∏ –≤—Ç–æ—Ä—É—é —Å—Ç–∞—Ç—å—é¬ª
‚Ä¢ ¬´–°—Ä–∞–≤–Ω–∏ —Å—Ç–∞—Ç—å–∏ 1 –∏ 2¬ª

–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç!"""
            action = "show_help"
        
        elif intent == Intent.GREETING:
            response_text = "üëã –ü—Ä–∏–≤–µ—Ç! –Ø AISA ‚Äî –≤–∞—à –Ω–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? –ù–∞–ø–∏—à–∏—Ç–µ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥."
            action = "greeting"
        
        else:
            # CHAT –∏–ª–∏ UNKNOWN ‚Äî –æ–±—ã—á–Ω—ã–π —á–∞—Ç
            context = await _nlu_pipeline.context_manager.get_context(user_id)
            response_text = await _chat_service.chat(message, context=context)
            action = "chat_response"
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—â—ë –Ω–µ –æ–±–Ω–æ–≤–∏–ª–∏)
        if action not in ["show_search_results"]:
            await _nlu_pipeline.update_context(
                user_id=user_id,
                message=message,
                result=nlu_result,
                bot_response=response_text
            )
        
        return ChatResponse(
            intent=intent.value,
            confidence=nlu_result.intent.confidence,
            entities=entities,
            response_text=response_text,
            action=action,
            data=data,
            needs_cloud_llm=nlu_result.needs_cloud_llm
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è")

@app.post("/api/v1/library/save")
async def save_paper_to_library(
    paper: Dict[str, Any],
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É"""
    try:
        user_id = current_user["user_id"]
        paper = paper.get('paper') or paper
        logger.debug(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –ø—ã—Ç–∞–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç—å—é: {paper}")
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å—é {paper['external_id']}")

        success = await db.save_paper(user_id, paper)
        if success:
            return {"message": "–°—Ç–∞—Ç—å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞", "success": True}
        else:
            return {"message": "–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏", "success": False}

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏")


@app.post("/api/v1/chat/test")
async def chat_test(chat_request: ChatRequest):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è —á–∞—Ç–∞ –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π user_id = 0 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    global _nlu_pipeline, _chat_service
    
    try:
        user_id = 0  # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        message = chat_request.message.strip()
        logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: '{message}'")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NLU Pipeline
        nlu_result = await _nlu_pipeline.process(user_id=user_id, message=message)
        
        intent = nlu_result.intent.intent
        entities = [
            {
                "type": e.type.value,
                "value": e.value,
                "confidence": e.confidence,
                "normalized": e.normalized_value
            }
            for e in nlu_result.entities.entities
        ]
        
        # –î–ª—è —Ç–µ—Å—Ç–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º NLU —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        response_text = ""
        if intent == Intent.SEARCH:
            query = nlu_result.query_params.get("query", message)
            response_text = f"üîç –†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø–æ–∏—Å–∫: ¬´{query}¬ª"
        elif intent == Intent.CHAT:
            context = await _nlu_pipeline.context_manager.get_context(user_id)
            response_text = await _chat_service.chat(message, context=context)
        else:
            response_text = f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω –∏–Ω—Ç–µ–Ω—Ç: {intent.value}"
        
        return ChatResponse(
            intent=intent.value,
            confidence=nlu_result.intent.confidence,
            entities=entities,
            response_text=response_text,
            action="test",
            data={"query_params": nlu_result.query_params},
            needs_cloud_llm=nlu_result.needs_cloud_llm
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/chat/stream")
async def chat_stream(
    chat_request: ChatRequest,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Streaming —á–∞—Ç —Å AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç Server-Sent Events (SSE) –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞.
    """
    global _nlu_pipeline, _chat_service
    
    user_id = current_user["user_id"]
    message = chat_request.message.strip()
    
    async def generate():
        try:
            # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ NLU
            nlu_result = await _nlu_pipeline.process(user_id=user_id, message=message)
            intent = nlu_result.intent.intent
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "event": "metadata",
                "intent": intent.value,
                "confidence": nlu_result.intent.confidence,
                "entities": [
                    {"type": e.type.value, "value": e.value}
                    for e in nlu_result.entities.entities
                ]
            }
            yield f"data: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            
            # –î–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º –µ–≥–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if intent == Intent.SEARCH:
                query = nlu_result.query_params.get("query", message)
                yield f"data: {json.dumps({'event': 'text', 'content': f'üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª...'}, ensure_ascii=False)}\n\n"
                
                try:
                    async with SearchService() as search_service:
                        search_results = await search_service.search_papers(query, limit=10)
                        search_results = search_service.aggregate_results(search_results, query)
                    
                    formatted_results = []
                    for paper in search_results:
                        paper_dict = paper.to_dict() if hasattr(paper, 'to_dict') else paper.__dict__
                        formatted_results.append(paper_dict)
                    
                    result_event = {
                        "event": "search_results",
                        "papers": formatted_results,
                        "query": query,
                        "count": len(formatted_results)
                    }
                    yield f"data: {json.dumps(result_event, ensure_ascii=False)}\n\n"
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                    yield f"data: {json.dumps({'event': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
            
            # –î–ª—è —á–∞—Ç–∞ —Å—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç
            elif intent in [Intent.CHAT, Intent.UNKNOWN, Intent.GREETING, Intent.HELP]:
                context = await _nlu_pipeline.context_manager.get_context(user_id)
                
                full_response = ""
                async for chunk in _chat_service.chat_stream(message, context=context):
                    full_response += chunk
                    yield f"data: {json.dumps({'event': 'text', 'content': chunk}, ensure_ascii=False)}\n\n"
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                await _nlu_pipeline.update_context(
                    user_id=user_id,
                    message=message,
                    result=nlu_result,
                    bot_response=full_response
                )
            
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ ‚Äî –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
                yield f"data: {json.dumps({'event': 'text', 'content': f'–†–∞—Å–ø–æ–∑–Ω–∞–Ω –∏–Ω—Ç–µ–Ω—Ç: {intent.value}'}, ensure_ascii=False)}\n\n"
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            yield f"data: {json.dumps({'event': 'done'})}\n\n"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ streaming: {e}", exc_info=True)
            yield f"data: {json.dumps({'event': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/api/v1/chat/stream/test")
async def chat_stream_test(chat_request: ChatRequest):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π streaming endpoint –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    global _nlu_pipeline, _chat_service
    
    user_id = 0
    message = chat_request.message.strip()
    
    async def generate():
        try:
            nlu_result = await _nlu_pipeline.process(user_id=user_id, message=message)
            intent = nlu_result.intent.intent
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "event": "metadata",
                "intent": intent.value,
                "confidence": nlu_result.intent.confidence,
                "query_params": nlu_result.query_params
            }
            yield f"data: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            
            if intent == Intent.SEARCH:
                query = nlu_result.query_params.get("query", message)
                yield f"data: {json.dumps({'event': 'text', 'content': f'üîç –ü–æ–∏—Å–∫: ¬´{query}¬ª'}, ensure_ascii=False)}\n\n"
                
                async with SearchService() as search_service:
                    search_results = await search_service.search_papers(query, limit=5)
                    search_results = search_service.aggregate_results(search_results, query)
                
                for paper in search_results:
                    paper_dict = paper.to_dict() if hasattr(paper, 'to_dict') else paper.__dict__
                    yield f"data: {json.dumps({'event': 'paper', 'paper': paper_dict}, ensure_ascii=False)}\n\n"
                
            elif intent in [Intent.CHAT, Intent.UNKNOWN]:
                context = await _nlu_pipeline.context_manager.get_context(user_id)
                async for chunk in _chat_service.chat_stream(message, context=context):
                    yield f"data: {json.dumps({'event': 'text', 'content': chunk}, ensure_ascii=False)}\n\n"
            
            else:
                yield f"data: {json.dumps({'event': 'text', 'content': f'Intent: {intent.value}'}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'event': 'done'})}\n\n"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {e}", exc_info=True)
            yield f"data: {json.dumps({'event': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
