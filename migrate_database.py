#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ö–µ–º—ã saved_publications.
–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –ø–æ–ª—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π Paper –∫–ª–∞—Å—Å–∞.
"""
import sqlite3
import json
import sys
import os
from typing import Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.logger import setup_logger

logger = setup_logger(name="migration_logger", log_file="logs/migration.log", level="INFO")

def backup_database(db_path: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    import shutil
    backup_path = f"{db_path}.backup"
    shutil.copy2(db_path, backup_path)
    logger.info(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_path}")
    return backup_path

def check_table_structure(db_path: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã saved_publications."""
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–±–ª–∏—Ü–µ
        cursor.execute("PRAGMA table_info(saved_publications)")
        columns = cursor.fetchall()
        
        column_names = [col[1] for col in columns]
        logger.info(f"–¢–µ–∫—É—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {column_names}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
        cursor.execute("SELECT COUNT(*) FROM saved_publications")
        count = cursor.fetchone()[0]
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ: {count}")
        
        return {
            'columns': column_names,
            'count': count,
            'needs_migration': not all(col in column_names for col in [
                'external_id', 'source', 'doi', 'journal', 'keywords', 'source_metadata'
            ])
        }

def migrate_database(db_path: str):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –º–∏–≥—Ä–∞—Ü–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    structure_info = check_table_structure(db_path)
    
    if not structure_info['needs_migration']:
        logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞, –º–∏–≥—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return True
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
    backup_database(db_path)
    
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        
        try:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS saved_publications_new (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    external_id TEXT,
                    source TEXT DEFAULT 'unknown',
                    title TEXT NOT NULL,
                    authors TEXT,
                    url TEXT NOT NULL,
                    abstract TEXT,
                    doi TEXT,
                    journal TEXT,
                    publication_date TEXT,
                    keywords TEXT,
                    saved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    tags TEXT, 
                    notes TEXT,
                    categories TEXT,
                    source_metadata TEXT,
                    UNIQUE (user_id, url)
                )
            ''')
            
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤ –Ω–æ–≤—É—é
            logger.info("–ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤ –Ω–æ–≤—É—é...")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã
            cursor.execute("SELECT * FROM saved_publications")
            old_rows = cursor.fetchall()
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã
            cursor.execute("PRAGMA table_info(saved_publications)")
            old_columns = [col[1] for col in cursor.fetchall()]
            
            for row in old_rows:
                old_data = dict(zip(old_columns, row))
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
                new_data = {
                    'user_id': old_data.get('user_id'),
                    'external_id': old_data.get('arxiv_id', old_data.get('external_id', '')),
                    'source': 'arxiv' if old_data.get('arxiv_id') else old_data.get('source', 'unknown'),
                    'title': old_data.get('title', ''),
                    'authors': old_data.get('authors', ''),
                    'url': old_data.get('url', ''),
                    'abstract': old_data.get('abstract', ''),
                    'doi': old_data.get('doi', ''),
                    'journal': old_data.get('journal', ''),
                    'publication_date': old_data.get('published_date', old_data.get('publication_date', '')),
                    'keywords': old_data.get('keywords', ''),
                    'saved_at': old_data.get('saved_at'),
                    'tags': old_data.get('tags', ''),
                    'notes': old_data.get('notes', ''),
                    'categories': old_data.get('categories', ''),
                    'source_metadata': old_data.get('source_metadata', '{}')
                }
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
                cursor.execute('''
                    INSERT INTO saved_publications_new (
                        user_id, external_id, source, title, authors, url, abstract, doi,
                        journal, publication_date, keywords, saved_at, tags, notes, categories, source_metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    new_data['user_id'], new_data['external_id'], new_data['source'],
                    new_data['title'], new_data['authors'], new_data['url'], new_data['abstract'],
                    new_data['doi'], new_data['journal'], new_data['publication_date'],
                    new_data['keywords'], new_data['saved_at'], new_data['tags'],
                    new_data['notes'], new_data['categories'], new_data['source_metadata']
                ))
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Ç–∞–±–ª–∏—Ü—É –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –Ω–æ–≤—É—é
            cursor.execute("DROP TABLE saved_publications")
            cursor.execute("ALTER TABLE saved_publications_new RENAME TO saved_publications")
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            logger.info("–°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã...")
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_id ON saved_publications(user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_external_id ON saved_publications(external_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON saved_publications(source)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_saved_at ON saved_publications(saved_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_doi ON saved_publications(doi)')
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            cursor.execute('''
                CREATE UNIQUE INDEX IF NOT EXISTS idx_user_paper 
                ON saved_publications(user_id, url)
            ''')
            
            conn.commit()
            logger.info(f"–ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ. –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(old_rows)} –∑–∞–ø–∏—Å–µ–π.")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")
            conn.rollback()
            return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∏–≥—Ä–∞—Ü–∏–∏
    final_structure = check_table_structure(db_path)
    logger.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å–ª–µ –º–∏–≥—Ä–∞—Ü–∏–∏: {final_structure['columns']}")
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –º–∏–≥—Ä–∞—Ü–∏–∏: {final_structure['count']}")
    
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏."""
    db_path = "db/scientific_assistant.db"
    
    if not os.path.exists(db_path):
        logger.error(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        return False
    
    print("=" * 60)
    print("–ú–ò–ì–†–ê–¶–ò–Ø –ë–ê–ó–´ –î–ê–ù–ù–´–• –°–û–•–†–ê–ù–ï–ù–ù–´–• –°–¢–ê–¢–ï–ô")
    print("=" * 60)
    print(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {db_path}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ –º–∏–≥—Ä–∞—Ü–∏—è
    structure_info = check_table_structure(db_path)
    
    if not structure_info['needs_migration']:
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞, –º–∏–≥—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return True
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏: {structure_info['count']}")
    print("üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º –º–∏–≥—Ä–∞—Ü–∏—é...")
    
    try:
        success = migrate_database(db_path)
        if success:
            print("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            print("\nüìã –ù–æ–≤—ã–µ –ø–æ–ª—è –≤ —Ç–∞–±–ª–∏—Ü–µ:")
            print("   - external_id: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–Ω–µ—à–Ω–∏–π ID (ArXiv, IEEE, PubMed)")
            print("   - source: –ò—Å—Ç–æ—á–Ω–∏–∫ —Å—Ç–∞—Ç—å–∏ (arxiv, ieee, ncbi)")
            print("   - doi: DOI —Å—Ç–∞—Ç—å–∏")
            print("   - journal: –ñ—É—Ä–Ω–∞–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            print("   - publication_date: –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–≤–º–µ—Å—Ç–æ published_date)")
            print("   - keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å—Ç–∞—Ç—å–∏")
            print("   - source_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (JSON)")
            print("\nüîß –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:")
            print("   - –ü–æ–ª–µ arxiv_id —Ç–µ–ø–µ—Ä—å –∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–∑ external_id –¥–ª—è ArXiv —Å—Ç–∞—Ç–µ–π")
            print("   - published_date –æ—Å—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω—ã–º –∫–∞–∫ –∞–ª–∏–∞—Å")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
            return False
            
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False
    
    print("\n" + "=" * 60)
    print("–ú–ò–ì–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 60)
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
