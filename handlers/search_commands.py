from aiogram import Dispatcher
from aiogram.types import Message
from aiogram.filters import Command
import re
from typing import Dict, Any, Optional
from services.search.semantic_scholar_service import SemanticScholarSearcher
from utils.metrics import track_operation
from services.search import ArxivSearcher, IEEESearcher, NCBISearcher
from services.search import SearchService
from utils.validators import InputValidator
from utils.error_handler import ErrorHandler
from utils.logger import setup_logger
import asyncio
from services.utils import SearchUtils

from config import TYPING_DELAY_SECONDS

logger = setup_logger(
    name="search_commands_logger",
    level="INFO"
)

validator = InputValidator()


def extract_search_filters(query: str) -> tuple[str, Dict[str, Any]]:
    """
    Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð· Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.
    
    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹:
    - year:2023 Ð¸Ð»Ð¸ year:"2023"
    - author:"John Smith" Ð¸Ð»Ð¸ author:smith
    
    Returns:
        tuple: (cleaned_query, filters_dict)
    """
    filters = {}
    cleaned_query = query
    def search_patterns(field: str, patterns: list[str], cleaned_query: str) -> Optional[str]:
        for pattern in patterns:
            match = re.search(pattern, cleaned_query, re.IGNORECASE)
            if match:
                filters[field] = match.group(1).strip()
                cleaned_query = re.sub(pattern, '', cleaned_query, flags=re.IGNORECASE)
        return cleaned_query
    
    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð° Ð¿Ð¾ Ð³Ð¾Ð´Ñƒ
    year_patterns = [
        r'year:(>[0-9]{4})', # Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ '>' + Ð³Ð¾Ð´
        r'year:(<[0-9]{4})', # Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ '<' + Ð³Ð¾Ð´
        r'year:([0-9]{4})',
        r'year:"([0-9]{4})"',
        r"year:'([0-9]{4})'"
    ]


    cleaned_query = search_patterns('year', year_patterns, cleaned_query)

    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð° Ð¿Ð¾ Ð°Ð²Ñ‚Ð¾Ñ€Ñƒ
    author_patterns = [
        r'author:"([^"]+)"',  # author:"John Smith"
        r"author:'([^']+)'",  # author:'John Smith'
        r'author:([^\s:]+)',    # author:smith
        r'au:"([^"]+)"',  # author:"John Smith"
        r"au:'([^']+)'",  # author:'John Smith'
        r'au:([^\s:]+)',    # author:smith
    ]
    cleaned_query = search_patterns('author', author_patterns, cleaned_query)

    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð° Ð¿Ð¾ Ð¶ÑƒÑ€Ð½Ð°Ð»Ñƒ
    journal_patterns = [
        r'journal:"([^"]+)"',
        r"journal:'([^']+)'",
        r'journal:([^\s:]+)',
        r'jr:"([^"]+)"',
        r"jr:'([^']+)'",
        r'jr:([^\s:]+)',
    ]
    cleaned_query = search_patterns('journal', journal_patterns, cleaned_query)

    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð° Ð¿Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ð¼ ÑÐ»Ð¾Ð²Ð°Ð¼
    citation_count_patterns = [
        r'citation_count:>(\d+)',  # citation_count:>100
        r'citation_count:<(\d+)',  # citation_count:<100
        r'citation_count:(\d+)',  # citation_count:100
        r'citation_count:"(\d+)"',  # citation_count:"100"
        r'citation_count:\'(\d+)\'',  # citation_count:'100'
        r'citation:>(\d+)',  # citation_count:>100
        r'citation:<(\d+)',  # citation_count:<100
        r'citation:(\d+)',  # citation_count:100
        r'citation:"(\d+)"',  # citation_count:"100"
        r'citation:\'(\d+)\'',  # citation_count:'100'
        
    ]
    cleaned_query = search_patterns('citation_count', citation_count_patterns, cleaned_query)

    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¾Ñ‚ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²
    cleaned_query = ' '.join(cleaned_query.split())
    
    logger.info(f"Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹: {filters}, Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ: {cleaned_query}")

    cleaned_query = cleaned_query.replace('--arxiv', '-a')
    cleaned_query = cleaned_query.replace('--ieee', '-i')
    cleaned_query = cleaned_query.replace('--ncbi', '-n')
    cleaned_query = cleaned_query.replace('--semantic_scholar', '-s')
    cleaned_query = cleaned_query.replace('--count', '-c')
    sources_patterns = [
        '-a', '-i', '-n', '-s'
    ]
    filters['source'] = []
    for source in sources_patterns:
        source_mapping = {
            '-a': 'arxiv',
            '-n': 'ncbi',
            '-i': '-ieee',
            '-s': 'semantic_scholar'
        }
        if source in cleaned_query:
            filters['source'].append(source_mapping.get(source))
            cleaned_query = cleaned_query.replace(source, '').strip()
    if not filters['source']:
        filters['source'] = None
    if '-c' in cleaned_query:
        filters['count'] = int(re.search(r'-c\s*(\d+)', cleaned_query).group(1))
        if filters['count'] < 1:
            filters['count'] = 1
        cleaned_query = cleaned_query.replace(f'-c {filters["count"]}', '').strip()

    return cleaned_query, filters

def register_search_handlers(dp: Dispatcher):
    dp.message.register(arxiv_command, Command("arxiv"))
    dp.message.register(ieee_command, Command("ieee"))
    dp.message.register(ncbi_command, Command("ncbi"))
    dp.message.register(search_command, Command("search"))
    dp.message.register(semantic_search_command, Command("semantic_search"))


@track_operation("arxiv_command")
async def arxiv_command(message: Message, **kwargs):
    """
    ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /arxiv - Ð¿Ð¾Ð¸ÑÐº Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… ÑÑ‚Ð°Ñ‚ÐµÐ¹
    
    Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ, Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ†Ð¸ÑŽ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð¾ÑˆÐ¸Ð±Ð¾Ðº
    """
    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
    query = message.text.replace("/arxiv ", "").strip()
    
    # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
    query, filters = extract_search_filters(query)
    
    query = validator.sanitize_text(query)
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚
    if validator.contains_suspicious_content(query):
        await ErrorHandler.handle_validation_error(
            message, 
            "ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°."
        )
        return
    
    # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
    if (not query or query.strip() == "/search") and filters.get('author') is None:
        await SearchUtils._send_search_help(message)
        return

    if len(query) < 3 and filters.get('author') is None:
        await ErrorHandler.handle_validation_error(
            message,
            "Ð—Ð°Ð¿Ñ€Ð¾Ñ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°."
        )
        return
    
    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¿Ð¾Ð¸ÑÐºÐ°
    await asyncio.sleep(TYPING_DELAY_SECONDS)
    await message.bot.send_chat_action(message.chat.id, "typing")
    status_message = await message.answer(f"ðŸ” Ð˜Ñ‰Ñƒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    limits = filters.get('count', 100)
    try:
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ð¾Ð¸ÑÐº
        async with ArxivSearcher() as searcher:
            papers = await searcher.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()
        
        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        await SearchUtils._send_search_results(message, papers, query, saved_urls)

    except Exception as e:
        await ErrorHandler.handle_search_error(message, e, status_message)
        
@track_operation("ieee_command")        
async def ieee_command(message: Message, **kwargs):
    """
    ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /ieee - Ð¿Ð¾Ð¸ÑÐº ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð² IEEE Xplore
    """
    try:
        query = message.text.replace("/ieee ", "").strip()
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        query, filters = extract_search_filters(query)
        
        query = validator.sanitize_text(query)
        
        if not query or len(query) < 3:
            await message.answer("âŒ ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ (Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°).")
            return
        
        await message.bot.send_chat_action(message.chat.id, "typing")
        status_message = await message.answer(f"ðŸ” Ð˜Ñ‰Ñƒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð² IEEE Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ: *{query}*...", parse_mode="Markdown")
        limits = filters.get('count', 100)
        async with IEEESearcher() as ieee_service:
            papers = await ieee_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()
        
        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        await SearchUtils._send_search_results(message, papers, query, saved_urls)
    
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð² IEEE: {e}")
        await ErrorHandler.handle_search_error(message, e)
        
@track_operation("ncbi_command")
async def ncbi_command(message: Message, **kwargs):
    """
    ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /ncbi - Ð¿Ð¾Ð¸ÑÐº ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð² NCBI
    """
    try:
        query = message.text.replace("/ncbi ", "").strip()
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        query, filters = extract_search_filters(query)
        
        query = validator.sanitize_text(query)

        if not query or len(query) < 3:
            await message.answer("âŒ ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ (Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°).")
            return

        await message.bot.send_chat_action(message.chat.id, "typing")
        status_message = await message.answer(f"ðŸ” Ð˜Ñ‰Ñƒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð² NCBI Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ: *{query}*...", parse_mode="Markdown")
        limits = filters.get('count', 100)
        async with NCBISearcher() as ncbi_service:
            papers = await ncbi_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()

        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        await SearchUtils._send_search_results(message, papers, query, saved_urls)

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð² NCBI: {e}")
        await ErrorHandler.handle_search_error(message, e)
        
@track_operation("search_command")
async def search_command(message: Message, **kwargs):
    """
    ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /search - ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ Ð²ÑÐµÐ¼ ÑÐµÑ€Ð²Ð¸ÑÐ°Ð¼
    """
    query = message.text.replace("/search ", "").strip()
    
    if not query:
        await SearchUtils._send_search_help(message)
        return
    
    # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
    query, filters = extract_search_filters(query)
    
    query = validator.sanitize_text(query)
    if not query or len(query) < 3:
        await SearchUtils._send_search_help(message)
        return
    
    await message.bot.send_chat_action(message.chat.id, "typing")
    status_message = await message.answer(f"ðŸ” Ð˜Ñ‰Ñƒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    
    try:
        limits = filters.get('count', 100)
        active_adapters = filters.get('source', None)
        async with SearchService() as search_service:
            results = await search_service.search_papers(query, limit=limits, services=active_adapters, filters=filters)

        await status_message.delete()
        
        if not results:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
        results = search_service.aggregate_results(results, query)
                # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        await SearchUtils._send_search_results(message, results, query, saved_urls)

    except Exception as e:
        await ErrorHandler.handle_search_error(message, e, status_message)
        
@track_operation("semantic_search_command")
async def semantic_search_command(message: Message, **kwargs):
    """
    ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° /semantic_search - Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ñƒ
    """
    query = message.text.replace("/semantic_search ", "").strip()
    if not query:
        await SearchUtils._send_search_help(message)
        return

    await message.bot.send_chat_action(message.chat.id, "typing")
    query, filters = extract_search_filters(query)
    query = validator.sanitize_text(query)
    status_message = await message.answer(f"ðŸ” Ð˜Ñ‰Ñƒ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    
    
    limits = filters.get('count', 100)
    if not query or len(query) < 3:
        await SearchUtils._send_search_help(message)
        return
    try:
        async with SemanticScholarSearcher() as search_service:
            results = await search_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()

        if not results:
            await SearchUtils._send_no_results_message(message, query)
            return

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        await SearchUtils._send_search_results(message, results, query, saved_urls)

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ ÑÑ‚Ð°Ñ‚ÐµÐ¹: {e}")
        await ErrorHandler.handle_search_error(message, e, status_message)
