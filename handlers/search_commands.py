from aiogram import Dispatcher
from aiogram.types import Message
from aiogram.filters import Command
import re
from typing import Dict, Any, Optional
from services.search.semantic_scholar_service import SemanticScholarSearcher
from utils.metrics import track_operation
from services.search import ArxivSearcher, IEEESearcher, NCBISearcher
from services.search import SearchService
from utils.validators import InputValidator
from utils.error_handler import ErrorHandler
from utils.logger import setup_logger
import asyncio
from services.utils import SearchUtils

from config import TYPING_DELAY_SECONDS

logger = setup_logger(
    name="search_commands_logger",
    level="INFO"
)

validator = InputValidator()


def extract_search_filters(query: str) -> tuple[str, Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã:
    - year:2023 –∏–ª–∏ year:"2023"
    - author:"John Smith" –∏–ª–∏ author:smith
    
    Returns:
        tuple: (cleaned_query, filters_dict)
    """
    filters = {}
    cleaned_query = query
    def search_patterns(field: str, patterns: list[str], cleaned_query: str) -> Optional[str]:
        for pattern in patterns:
            match = re.search(pattern, cleaned_query, re.IGNORECASE)
            if match:
                filters[field] = match.group(1).strip()
                cleaned_query = re.sub(pattern, '', cleaned_query, flags=re.IGNORECASE)
        return cleaned_query
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –≥–æ–¥—É
    year_patterns = [
        r'year:(>[0-9]{4})', # –≤–∫–ª—é—á–∞–µ—Ç '>' + –≥–æ–¥
        r'year:(<[0-9]{4})', # –≤–∫–ª—é—á–∞–µ—Ç '<' + –≥–æ–¥
        r'year:([0-9]{4})',
        r'year:"([0-9]{4})"',
        r"year:'([0-9]{4})'"
    ]


    cleaned_query = search_patterns('year', year_patterns, cleaned_query)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –∞–≤—Ç–æ—Ä—É
    author_patterns = [
        r'author:"([^"]+)"',  # author:"John Smith"
        r"author:'([^']+)'",  # author:'John Smith'
        r'author:([^\s:]+)',    # author:smith
        r'au:"([^"]+)"',  # author:"John Smith"
        r"au:'([^']+)'",  # author:'John Smith'
        r'au:([^\s:]+)',    # author:smith
    ]
    cleaned_query = search_patterns('author', author_patterns, cleaned_query)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –∂—É—Ä–Ω–∞–ª—É
    journal_patterns = [
        r'journal:"([^"]+)"',
        r"journal:'([^']+)'",
        r'journal:([^\s:]+)',
        r'jr:"([^"]+)"',
        r"jr:'([^']+)'",
        r'jr:([^\s:]+)',
    ]
    cleaned_query = search_patterns('journal', journal_patterns, cleaned_query)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    citation_count_patterns = [
        r'citation_count:>(\d+)',  # citation_count:>100
        r'citation_count:<(\d+)',  # citation_count:<100
        r'citation_count:(\d+)',  # citation_count:100
        r'citation_count:"(\d+)"',  # citation_count:"100"
        r'citation_count:\'(\d+)\'',  # citation_count:'100'
        r'citation:>(\d+)',  # citation_count:>100
        r'citation:<(\d+)',  # citation_count:<100
        r'citation:(\d+)',  # citation_count:100
        r'citation:"(\d+)"',  # citation_count:"100"
        r'citation:\'(\d+)\'',  # citation_count:'100'
        
    ]
    cleaned_query = search_patterns('citation_count', citation_count_patterns, cleaned_query)

    # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    cleaned_query = ' '.join(cleaned_query.split())
    
    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã: {filters}, –æ—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {cleaned_query}")

    cleaned_query = cleaned_query.replace('--arxiv', '-a')
    cleaned_query = cleaned_query.replace('--ieee', '-i')
    cleaned_query = cleaned_query.replace('--ncbi', '-n')
    cleaned_query = cleaned_query.replace('--semantic_scholar', '-s')
    cleaned_query = cleaned_query.replace('--count', '-c')
    sources_patterns = [
        '-a', '-i', '-n', '-s'
    ]
    filters['source'] = []
    for source in sources_patterns:
        source_mapping = {
            '-a': 'arxiv',
            '-n': 'ncbi',
            '-i': '-ieee',
            '-s': 'semantic_scholar'
        }
        if source in cleaned_query:
            filters['source'].append(source_mapping.get(source))
            cleaned_query = cleaned_query.replace(source, '').strip()
    if not filters['source']:
        filters['source'] = None
    if '-c' in cleaned_query:
        filters['count'] = int(re.search(r'-c\s*(\d+)', cleaned_query).group(1))
        if filters['count'] < 1:
            filters['count'] = 1
        cleaned_query = cleaned_query.replace(f'-c {filters["count"]}', '').strip()

    return cleaned_query, filters

def register_search_handlers(dp: Dispatcher):
    dp.message.register(arxiv_command, Command("arxiv"))
    dp.message.register(ieee_command, Command("ieee"))
    dp.message.register(ncbi_command, Command("ncbi"))
    dp.message.register(search_command, Command("search"))
    dp.message.register(semantic_search_command, Command("semantic_search"))


async def perform_search(
    message: Message,
    query: str,
    filters: Optional[Dict[str, Any]] = None,
    source: Optional[str] = None,
) -> bool:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∫–æ–º–∞–Ω–¥–∞–º–∏ (/search, /arxiv –∏ —Ç.–¥.), 
    —Ç–∞–∫ –∏ NLU-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–º (chat_handler).
    
    Args:
        message: Telegram Message –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        filters: –§–∏–ª—å—Ç—Ä—ã (year, author, journal –∏ —Ç.–¥.)
        source: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (arxiv, ieee, ncbi, semantic_scholar)
        
    Returns:
        bool: True –µ—Å–ª–∏ –ø–æ–∏—Å–∫ —É—Å–ø–µ—à–µ–Ω, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    filters = filters or {}
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
    query = validator.sanitize_text(query)
    
    if not query or len(query) < 2:
        await SearchUtils._send_search_help(message)
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    if validator.contains_suspicious_content(query):
        await message.answer(
            "‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞."
        )
        return False
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞
    await message.bot.send_chat_action(message.chat.id, "typing")
    status_message = await message.answer(
        f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{validator.escape_markdown(query)}*...",
        parse_mode="Markdown"
    )
    
    try:
        limits = filters.get('count', 100)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞
        if source:
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            source_lower = source.lower()
            if source_lower == 'arxiv':
                async with ArxivSearcher() as searcher:
                    papers = await searcher.search_papers(query, limit=limits, filters=filters)
            elif source_lower == 'ieee':
                async with IEEESearcher() as searcher:
                    papers = await searcher.search_papers(query, limit=limits, filters=filters)
            elif source_lower == 'ncbi' or source_lower == 'pubmed':
                async with NCBISearcher() as searcher:
                    papers = await searcher.search_papers(query, limit=limits, filters=filters)
            elif source_lower == 'semantic_scholar':
                async with SemanticScholarSearcher() as searcher:
                    papers = await searcher.search_papers(query, limit=limits, filters=filters)
            else:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫
                async with SearchService() as search_service:
                    papers = await search_service.search_papers(query, limit=limits, filters=filters)
                    papers = search_service.aggregate_results(papers, query)
        else:
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            active_adapters = filters.get('source', None)
            async with SearchService() as search_service:
                papers = await search_service.search_papers(
                    query, limit=limits, services=active_adapters, filters=filters
                )
                papers = search_service.aggregate_results(papers, query)
        
        await status_message.delete()
        
        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return True  # –ü–æ–∏—Å–∫ —É—Å–ø–µ—à–µ–Ω, –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, papers, query, saved_urls)
        
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        await ErrorHandler.handle_search_error(message, e, status_message)
        return False


@track_operation("arxiv_command")
async def arxiv_command(message: Message, **kwargs):
    """
    –ö–æ–º–∞–Ω–¥–∞ /arxiv - –ø–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    
    –í–∫–ª—é—á–∞–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é, –∏–Ω–¥–∏–∫–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
    """
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
    query = message.text.replace("/arxiv ", "").strip()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query, filters = extract_search_filters(query)
    
    query = validator.sanitize_text(query)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    if validator.contains_suspicious_content(query):
        await ErrorHandler.handle_validation_error(
            message, 
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞."
        )
        return
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
    if (not query or query.strip() == "/search") and filters.get('author') is None:
        await SearchUtils._send_search_help(message)
        return

    if len(query) < 3 and filters.get('author') is None:
        await ErrorHandler.handle_validation_error(
            message,
            "–ó–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞."
        )
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞
    await asyncio.sleep(TYPING_DELAY_SECONDS)
    await message.bot.send_chat_action(message.chat.id, "typing")
    status_message = await message.answer(f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    limits = filters.get('count', 100)
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        async with ArxivSearcher() as searcher:
            papers = await searcher.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()
        
        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, papers, query, saved_urls)

    except Exception as e:
        await ErrorHandler.handle_search_error(message, e, status_message)
        
@track_operation("ieee_command")        
async def ieee_command(message: Message, **kwargs):
    """
    –ö–æ–º–∞–Ω–¥–∞ /ieee - –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ IEEE Xplore
    """
    try:
        query = message.text.replace("/ieee ", "").strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query, filters = extract_search_filters(query)
        
        query = validator.sanitize_text(query)
        
        if not query or len(query) < 3:
            await message.answer("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞).")
            return
        
        await message.bot.send_chat_action(message.chat.id, "typing")
        status_message = await message.answer(f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –≤ IEEE –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{query}*...", parse_mode="Markdown")
        limits = filters.get('count', 100)
        async with IEEESearcher() as ieee_service:
            papers = await ieee_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()
        
        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, papers, query, saved_urls)
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π –≤ IEEE: {e}")
        await ErrorHandler.handle_search_error(message, e)
        
@track_operation("ncbi_command")
async def ncbi_command(message: Message, **kwargs):
    """
    –ö–æ–º–∞–Ω–¥–∞ /ncbi - –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ NCBI
    """
    try:
        query = message.text.replace("/ncbi ", "").strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query, filters = extract_search_filters(query)
        
        query = validator.sanitize_text(query)

        if not query or len(query) < 3:
            await message.answer("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞).")
            return

        await message.bot.send_chat_action(message.chat.id, "typing")
        status_message = await message.answer(f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –≤ NCBI –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{query}*...", parse_mode="Markdown")
        limits = filters.get('count', 100)
        async with NCBISearcher() as ncbi_service:
            papers = await ncbi_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()

        if not papers:
            await SearchUtils._send_no_results_message(message, query)
            return

        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, papers, query, saved_urls)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π –≤ NCBI: {e}")
        await ErrorHandler.handle_search_error(message, e)
        
@track_operation("search_command")
async def search_command(message: Message, **kwargs):
    """
    –ö–æ–º–∞–Ω–¥–∞ /search - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–µ—Ä–≤–∏—Å–∞–º
    """
    query = message.text.replace("/search ", "").strip()
    
    if not query:
        await SearchUtils._send_search_help(message)
        return
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query, filters = extract_search_filters(query)
    
    query = validator.sanitize_text(query)
    if not query or len(query) < 3:
        await SearchUtils._send_search_help(message)
        return
    
    await message.bot.send_chat_action(message.chat.id, "typing")
    status_message = await message.answer(f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    
    try:
        limits = filters.get('count', 100)
        active_adapters = filters.get('source', None)
        async with SearchService() as search_service:
            results = await search_service.search_papers(query, limit=limits, services=active_adapters, filters=filters)

        await status_message.delete()
        
        if not results:
            await SearchUtils._send_no_results_message(message, query)
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
        results = search_service.aggregate_results(results, query)
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, results, query, saved_urls)

    except Exception as e:
        await ErrorHandler.handle_search_error(message, e, status_message)
        
@track_operation("semantic_search_command")
async def semantic_search_command(message: Message, **kwargs):
    """
    –ö–æ–º–∞–Ω–¥–∞ /semantic_search - –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
    """
    query = message.text.replace("/semantic_search ", "").strip()
    if not query:
        await SearchUtils._send_search_help(message)
        return

    await message.bot.send_chat_action(message.chat.id, "typing")
    query, filters = extract_search_filters(query)
    query = validator.sanitize_text(query)
    status_message = await message.answer(f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –∑–∞–ø—Ä–æ—Å—É: *{validator.escape_markdown(query)}*...", parse_mode="Markdown")
    
    
    limits = filters.get('count', 100)
    if not query or len(query) < 3:
        await SearchUtils._send_search_help(message)
        return
    try:
        async with SemanticScholarSearcher() as search_service:
            results = await search_service.search_papers(query, limit=limits, filters=filters)

        await status_message.delete()

        if not results:
            await SearchUtils._send_no_results_message(message, query)
            return

        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await SearchUtils._send_search_results(message, results, query, saved_urls)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π: {e}")
        await ErrorHandler.handle_search_error(message, e, status_message)
