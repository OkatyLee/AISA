"""
DEPRECATED: –≠—Ç–æ—Ç –º–æ–¥—É–ª—å —É—Å—Ç–∞—Ä–µ–ª –∏ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏.
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ handlers.chat_handler –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ.

–î–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–º–ø–æ—Ä—Ç–æ–≤:
    OLD: from handlers.messages import MessageHandler
    NEW: from handlers.chat_handler import handle_message
"""

import warnings
warnings.warn(
    "handlers.messages is deprecated. Use handlers.chat_handler instead.",
    DeprecationWarning,
    stacklevel=2
)

import os
from aiogram import Dispatcher, F
from aiogram.types import Message, FSInputFile
from config.messages import COMMAND_MESSAGES
from utils.validators import InputValidator
from nlp.query_processor import QueryProcessingResult, QueryProcessor
from nlp.context_manager import ContextManager
from utils.nlu.intents import Intent
from utils.logger import setup_logger
from utils.error_handler import ErrorHandler
from .search_commands import extract_search_filters

validator = InputValidator()
query_processor = QueryProcessor()
context_manager = ContextManager("db/scientific_assistant.db")
logger = setup_logger(
    name='messages_logger',
    level='DEBUG'
)

# Backward-compatible shim for tests expecting class-based handler
class MessageHandler:
    def __init__(self, *args, **kwargs):
        # args are ignored; modern flow uses nlp.query_processor internally
        pass

    async def handle(self, text: str) -> str:
        # Minimal shim: mimic processing a generic message without Telegram context
        # This is for test suite compatibility only.
        fake = type("_Msg", (), {})()
        fake.text = text
        fake.from_user = type("_U", (), {"id": 0})()
        fake.chat = type("_C", (), {"id": 0})()

        # Provide minimal answer method to collect response
        responses = []
        async def _answer(t, **kwargs):
            responses.append(str(t))
        async def _answer_document(doc, **kwargs):
            # record that a document would be sent
            responses.append(f"<document:{getattr(doc, 'path', 'file')}>")
        fake.answer = _answer
        fake.answer_document = _answer_document

        # Route into existing pipeline with a default intent path
        try:
            result = await query_processor.process(text, {})
            await _handle_processed_query(fake, result)
        except Exception as _:
            # fallback generic message
            responses.append("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ")
        # Return last response text for tests
        return responses[-1] if responses else ""

def register_message_handlers(dp: Dispatcher):
    
    dp.message.register(message_handler, F.text)

async def message_handler(message: Message):
    
    text = validator.sanitize_text(message.text)
    user_id = message.from_user.id

    if validator.contains_suspicious_content(text):
        await message.answer(
            "‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. "
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã."
        )
        return

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    try:
        user_context = await context_manager.get_user_context(user_id)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é NLP —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        result = await query_processor.process(text, user_context)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        bot_response = await _handle_processed_query(message, result)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        await context_manager.update_user_context(
            user_id=user_id,
            message=text,
            intent=result.intent.intent,
            entities=result.entities.entities,
            bot_response=bot_response,
            search_results=result.query_params.get('search_results', [])
        )
        
    except Exception as e:
        await ErrorHandler.handle_message_error(message, e, status_message=None)

async def _handle_processed_query(message: Message, result: QueryProcessingResult) -> str:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç NLP-–∞–Ω–∞–ª–∏–∑–∞ –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
    
    Returns:
        str: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –±–æ—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    """
    intent = result.intent.intent
    params = result.query_params
    
    if intent == Intent.SEARCH:
        return await _handle_search_intent(message, params)
    elif intent == Intent.GREETING:
        return await _handle_greeting_intent(message)
    elif intent == Intent.HELP:
        return await _handle_help_intent(message)
    elif intent == Intent.LIST_SAVED:
        return await _handle_list_saved_intent(message)
    elif intent == Intent.GET_SUMMARY:
        return await _handle_summary_intent(message, params)
    elif intent == Intent.UNKNOWN:
        return await _handle_unknown_intent(message, result)
    else:
        response = ("–Ø –ø–æ–Ω—è–ª –≤–∞—à–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ, –Ω–æ –ø–æ–∫–∞ –Ω–µ —É–º–µ—é —ç—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å. "
                   "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å.")
        await message.answer(response)
        return response

async def _handle_search_intent(message: Message, params: dict) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞.
    Args:
        message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    
    Returns:
        str: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –±–æ—Ç–∞
    """
    logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
    try:
        if "query" in params:
            query = params["query"]
        elif "topic" in params:
            query = params["topic"]
        else:
            query = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"  # default query

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        filters = {}
        if "year" in params:
            filters["year"] = params["year"]
        if "author" in params:
            filters["author"] = params["author"]
        
        # –¢–∞–∫–∂–µ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ (—Å–∏–Ω—Ç–∞–∫—Å–∏—Å year:2023, author:"Name")
        original_query = query
        query, additional_filters = extract_search_filters(query)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Ç–µ–∫—Å—Ç–∞
        filters.update(additional_filters)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_command_text = f"/search {query}"
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç
        filter_info = ""
        if filters:
            filter_parts = []
            if "year" in filters:
                filter_parts.append(f"–≥–æ–¥: {filters['year']}")
            if "author" in filters:
                filter_parts.append(f"–∞–≤—Ç–æ—Ä: {filters['author']}")
            filter_info = f" (—Ñ–∏–ª—å—Ç—Ä—ã: {', '.join(filter_parts)})"
        
        search_response = f"üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}{filter_info}"
        await message.answer(search_response)
        
        from .search_commands import search_command
        from services.search import SearchService
        from services.utils import SearchUtils
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –Ω–∞–ø—Ä—è–º—É—é —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
        try:
            search_service = SearchService()
            results = await search_service.search_papers(query, limit=5, filters=filters)
            
            if not results or not any(result.success for result in results.values()):
                await SearchUtils._send_no_results_message(message, original_query)
                return search_response
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            saved_urls = await SearchUtils._get_user_saved_urls(message.from_user.id)
            aggregated_results = search_service.aggregate_results(results)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            await SearchUtils._send_search_results(message, aggregated_results, query, saved_urls)
            
        except Exception as search_error:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ NLP: {search_error}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –∫–æ–º–∞–Ω–¥—É –ø–æ–∏—Å–∫–∞
            message = message.model_copy(update={"text": search_command_text})
            await search_command(message)
        
        return search_response
        
    except Exception as e:
        error_response = "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        await ErrorHandler.handle_search_error(message, e)
        return error_response

async def _handle_greeting_intent(message: Message) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ."""
    response = (
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –Ω–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç AISA.\n\n"
        "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º:\n"
        "üîç –ù–∞–π—Ç–∏ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
        "üìö –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
        "üìù –ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ/–∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π\n"
        "üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –º–Ω–µ, —á—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã!"
    )
    await message.answer(response)
    return response

async def _handle_help_intent(message: Message) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–º–æ—â–∏."""
    response = COMMAND_MESSAGES.get("help_text", "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å –ø–æ–∏—Å–∫–æ–º –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç–∞—Ç–µ–π. –í–æ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥:")
    await message.answer(response)
    return response

async def _handle_list_saved_intent(message: Message) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π."""
    try:
        from .commands import library_command
        await library_command(message)
        return "–ü–æ–∫–∞–∑—ã–≤–∞—é –≤–∞—à–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"
        
    except Exception as e:
        error_response = "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"
        await ErrorHandler.handle_library_error(message, e)
        return error_response

async def _handle_summary_intent(message: Message, params: dict) -> str:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Ä–µ–∑—é–º–µ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: URL, DOI, arXiv ID, PubMed ID, IEEE ID –∏–ª–∏ —Å–≤–æ–±–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Å—Å—ã–ª–∫–æ–π/ID.
    """
    try:
        from services.search import SearchService
        from services.nlp import LLMService
        from services.utils.paper import Paper
        from nlp.entity_classifier import RuleBasedEntityExtractor
        from utils.nlu.intents import Intent as _Intent
        from services.utils.search_utils import SearchUtils
        from utils.report import save_md_and_pdf, delete_report_files

        user_id = message.from_user.id
        identifier = None
        id_type = None
        raw_text = params.get("query") or ""
        text_lower = raw_text.lower()
        compare_request = any(x in text_lower for x in [
            "—Å—Ä–∞–≤–Ω", "compare", "–Ω–µ—Å–∫–æ–ª—å–∫–æ", "–æ–±–∞", "–¥–≤–µ", "–¥–≤—É—Ö", "3 —Å—Ç–∞—Ç—å–∏", "–Ω–µ—Å–∫ —Å—Ç–∞—Ç", "—Å—Ä–∞–≤–Ω–∏"
        ])

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —è–≤–Ω—ã–µ –ø–æ–ª—è –≤ params
        for key in ["url", "doi", "arxiv_id", "pubmed_id", "ieee_id"]:
            if key in params and params[key]:
                identifier = params[key]
                id_type = key
                break

        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        identifiers: list[tuple[str, str]] = []
        if raw_text:
            extractor = RuleBasedEntityExtractor()
            extracted = await extractor.extract(raw_text, _Intent.GET_SUMMARY)
            for e in extracted.entities:
                if e.type.value in ["url", "doi", "arxiv_id", "pubmed_id", "ieee_id"]:
                    identifiers.append((e.type.value, str(e.normalized_value or e.value)))
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {identifiers}")
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        if identifiers:
            seen = set()
            uniq = []
            for t, v in identifiers:
                k = (t, v)
                if k not in seen:
                    seen.add(k)
                    uniq.append((t, v))
            identifiers = uniq
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: {identifiers}")
        # –ú—É–ª—å—Ç–∏-–∞–Ω–∞–ª–∏–∑, –µ—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –Ω–∞—à–ª–∏ >=2 id
        if compare_request and len(identifiers) >= 2:
            async with SearchService() as searcher:
                processing_msg = await message.answer("üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
                type_map = {
                    "url": "url",
                    "doi": "doi",
                    "arxiv_id": "arxiv",
                    "pubmed_id": "pubmed",
                    "ieee_id": "ieee",
                }
                papers: list[Paper] = []
                for t, v in identifiers[:5]:
                    cb_type = type_map.get(t, "url")
                    try:
                        p = await searcher.get_paper_by_identifier(cb_type, v, user_id)
                        if isinstance(p, Paper):
                            papers.append(p)
                    except Exception:
                        continue
                if not papers:
                    await message.answer("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
                    return "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
                try:
                    items = await searcher.fetch_full_texts_for_papers(papers)
                    if processing_msg:
                        await processing_msg.edit_text("‚è≥ –ì–æ—Ç–æ–≤–ª—é —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π‚Ä¶")
                    else:
                        processing_msg = await message.answer("‚è≥ –ì–æ—Ç–æ–≤–ª—é —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π‚Ä¶")
                    async with LLMService() as llm_service:
                        summary = await llm_service.compare_many(items)
                finally:
                    try:
                        await processing_msg.delete()
                    except Exception:
                        pass
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç: MD –≤—Å–µ–≥–¥–∞, PDF –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
                base_name = "comparison_report"
                await processing_msg.edit_text("üìÑ –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç")
                if summary == "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.":
                    await processing_msg.edit_text("‚ùå " + summary)
                    return "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                md_path, pdf_path = save_md_and_pdf(summary, base_name)
                await processing_msg.delete()
                if pdf_path and os.path.isfile(pdf_path) and os.path.getsize(pdf_path) > 0:
                    await message.answer_document(FSInputFile(pdf_path), caption="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (PDF)")
                else:
                    await message.answer_document(FSInputFile(md_path), caption="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (Markdown)")
                delete_report_files(base_name)
                return "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω"

        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        if not identifier:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø–æ–∏—Å–∫—É
            if compare_request and not identifiers:
                sid, data = SearchUtils._get_last_active_search(user_id)
                papers = []
                if data and data.get('papers'):
                    papers = data['papers'][:3]
                if papers:
                    processing_msg = await message.answer("üîç –ò—â—É —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç—å–µ–π –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
                    async with SearchService() as searcher:
                        items = await searcher.fetch_full_texts_for_papers(papers)
                        async with LLMService() as llm_service:
                            await processing_msg.edit_text("‚è≥ –ì–æ—Ç–æ–≤–ª—é —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π‚Ä¶")
                            summary = await llm_service.compare_many(items)
                    
                    base_name = "comparison_report"
                    if summary == "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.":
                        await processing_msg.edit_text("‚ùå " + summary)
                        return "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                    md_path, pdf_path = save_md_and_pdf(summary, base_name)
                    logger.debug(f'MD: {md_path}, exists= {os.path.isfile(md_path)}, size= {os.path.getsize(md_path) if os.path.isfile(md_path) else 0}')
                    if pdf_path and os.path.isfile(pdf_path) and os.path.getsize(pdf_path) > 0:
                        await message.answer_document(FSInputFile(pdf_path), caption="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (PDF)")
                    else:
                        await message.answer_document(FSInputFile(md_path), caption="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (Markdown)")
                    await processing_msg.delete()
                    delete_report_files(base_name)
                    return "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω"

            # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
            try:
                current_paper = SearchUtils.get_current_paper_for_user(user_id)
            except Exception:
                current_paper = None
            if current_paper:
                processing_msg = await message.answer("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—É—â—É—é –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é‚Ä¶")
                async with LLMService() as llm_service:
                    
                    summary = await llm_service.summarize(current_paper)
                    
                base_name = "article_analysis"
                if summary == "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.":
                    await processing_msg.edit_text("‚ùå " + summary)
                    return "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                md_path, pdf_path = save_md_and_pdf(summary, base_name)
                if pdf_path and os.path.isfile(pdf_path) and os.path.getsize(pdf_path) > 0:
                    await message.answer_document(FSInputFile(pdf_path), caption="–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ (PDF)")
                else:
                    await message.answer_document(FSInputFile(md_path), caption="–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ (Markdown)")
                delete_report_files(base_name)
                return "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω"

            # –ü—Ä–æ—Å–∏–º —É–∫–∞–∑–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            response = (
                "–ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –∞–Ω–∞–ª–∏–∑, –ø—Ä–∏—à–ª–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –µ—ë DOI/ID (arXiv, PubMed, IEEE)."
            )
            await message.answer(response)
            return response

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å—é –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É (–æ–¥–Ω–∞ —Å—Ç–∞—Ç—å—è)
        async with SearchService() as searcher:
            type_map = {
                "url": "url",
                "doi": "doi",
                "arxiv_id": "arxiv",
                "pubmed_id": "pubmed",
                "ieee_id": "ieee",
            }
            processing_msg = await message.answer("üîç –ò—â—É –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏‚Ä¶")
            callback_type = type_map.get(id_type, "url")
            paper = await searcher.get_paper_by_identifier(callback_type, str(identifier), user_id, full_text=True)

        if not paper:
            response = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–µ/ID."
            await message.answer(response)
            return response
        if processing_msg:
            await processing_msg.edit_text("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç–∞—Ç—å—é, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è‚Ä¶")
        else:
            processing_msg = await message.answer("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç–∞—Ç—å—é, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è‚Ä¶")
        async with LLMService() as llm_service:
            summary = await llm_service.summarize(paper)
        
        base_name = "article_analysis"
        if summary == "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.":
            await processing_msg.edit_text("‚ùå " + summary)
            return "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ–Ω—å –∏—Å—á–µ—Ä–ø–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        md_path, pdf_path = save_md_and_pdf(summary, base_name)
        await processing_msg.edit_text("üìÑ –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç")
        logger.debug(f"MD: {md_path}, exists={os.path.isfile(md_path)}, size={os.path.getsize(md_path) if os.path.isfile(md_path) else 0}")
        logger.debug(f"PDF: {pdf_path}, exists={os.path.isfile(pdf_path) if pdf_path else False}, size={os.path.getsize(pdf_path) if pdf_path and os.path.isfile(pdf_path) else 0}")
        if processing_msg:
            try:
                await processing_msg.delete()
            except Exception:
                pass
        if pdf_path and os.path.isfile(pdf_path) and os.path.getsize(pdf_path) > 0:
            await message.answer_document(FSInputFile(pdf_path), caption="–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ (PDF)")
        else:
            await message.answer_document(FSInputFile(md_path), caption="–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ (Markdown)")
        delete_report_files(base_name)
        return "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω"
    except Exception as e:
        await ErrorHandler.handle_summarization_error(message, e)
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ"

async def _handle_unknown_intent(message: Message, result) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ."""
    confidence = result.intent.confidence
    
    if confidence < 0.3:
        response = (
            "ü§î –Ø –Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å.\n\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
            "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã (/help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)\n"
            "‚Ä¢ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å\n"
            "‚Ä¢ –ë—ã—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º\n\n"
            "–ù–∞–ø—Ä–∏–º–µ—Ä: \"–ù–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ machine learning\" –∏–ª–∏ \"–ú–æ–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\"\n"
            "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n"
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –∞–≤—Ç–æ—Ä–æ–≤ –ª—É—á—à–µ –ø–∏—Å–∞—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ."
        )
    else:
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Å —Ö–æ—Ä–æ—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        alternatives = result.intent.alternatives
        if alternatives and alternatives[0][1] > 0.2:
            alt_intent = alternatives[0][0]
            response = f"ü§î –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã —Ö–æ—Ç–µ–ª–∏: {_intent_to_text(alt_intent)}?\n"
            response += "–ï—Å–ª–∏ –¥–∞, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å –±–æ–ª–µ–µ —á–µ—Ç–∫–æ."
        else:
            response = (
                "–Ø –Ω–µ —É–≤–µ—Ä–µ–Ω, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å. "
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±—ã—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã."
            )
    
    await message.answer(response)
    return response

def _intent_to_text(intent: Intent) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."""
    intent_texts = {
        Intent.SEARCH: "–Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏",
        Intent.LIST_SAVED: "–ø–æ–∫–∞–∑–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏",
        Intent.HELP: "–ø–æ–ª—É—á–∏—Ç—å –ø–æ–º–æ—â—å",
        Intent.GREETING: "–ø–æ–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è",
        Intent.GET_SUMMARY: "–ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç—å–∏"
    }
    return intent_texts.get(intent, "—á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ")