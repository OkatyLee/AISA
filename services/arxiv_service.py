import httpx
from config.config import load_config
from config.constants import ARXIV_API_BASE_URL, ARXIV_NAMESPACES, API_TIMEOUT_SECONDS
from typing import List, Dict, Any, Optional
import xml.etree.ElementTree as ET
from datetime import datetime
from utils import setup_logger
from utils.metrics import metrics
import logging
from aiogram.utils.markdown import hbold, hitalic, hlink
from urllib.parse import urlparse
import re

logger = setup_logger(name="arxiv_service_logger", log_file="logs/arxiv_service.log", level=logging.INFO)

class ArxivSearcher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ArXiv API
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ API
    """

    def __init__(self):
        self.session = None
        self.config = load_config()
        self.MAX_RESULTS = self.config.MAX_RESULTS
        self._cache = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    async def __aenter__(self):
        self.session = httpx.AsyncClient(
            timeout=30.0,
            follow_redirects=True
        )
        return self
    
    async def __aexit__(self, exc_type, exc_value, traceback):
        if self.session:
            await self.session.aclose()

    async def search_papers(self, query: str) -> List[Dict[str, str]]:
        """
        –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ ArXiv API —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
        """
        if not self.session:
            raise ValueError("ArxivSearcher is not initialized")
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"search_{hash(query)}_{self.MAX_RESULTS}"
        if cache_key in self._cache:
            logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            metrics.record_operation("arxiv_search_cache_hit", 0, None, True)
            return self._cache[cache_key]
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
        search_start_time = datetime.now()
        
        try:
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            url = ARXIV_API_BASE_URL
            params = {
                'search_query': self._build_search_query(query),
                'start': 0,
                'sortBy': 'relevance',
                'sortOrder': 'descending',
                "max_results": self.MAX_RESULTS 
            }

            logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ ArXiv —Å –∑–∞–ø—Ä–æ—Å–æ–º: {params['search_query']}")
            response = await self.session.get(url, params=params)
            response.raise_for_status()

            papers = self._parse_arxiv_response(response.text)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._cache[cache_key] = papers
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é
            search_duration = (datetime.now() - search_start_time).total_seconds()
            metrics.record_operation("arxiv_search_success", 0, search_duration, True)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(papers)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            
            return papers
            
        except httpx.HTTPStatusError as e:
            search_duration = (datetime.now() - search_start_time).total_seconds()
            metrics.record_operation("arxiv_search_http_error", 0, search_duration, False)
            logger.error(f"HTTP –æ—à–∏–±–∫–∞: {e.response.status_code} - {e.response.text}")
            return []
        except httpx.TimeoutException as e:
            search_duration = (datetime.now() - search_start_time).total_seconds()
            metrics.record_operation("arxiv_search_timeout", 0, search_duration, False)
            logger.error(f"–í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏—Å—Ç–µ–∫–ª–æ: {e}")
            return []
        except httpx.ConnectError as e:
            search_duration = (datetime.now() - search_start_time).total_seconds()
            metrics.record_operation("arxiv_search_connection_error", 0, search_duration, False)
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
            return []
        except Exception as e:
            search_duration = (datetime.now() - search_start_time).total_seconds()
            metrics.record_operation("arxiv_search_unknown_error", 0, search_duration, False)
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return []
    
    def _build_search_query(self, query: str) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è ArXiv API
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è ArXiv API
        """
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean_query = re.sub(r'[^\w\s\-]', ' ', query).strip()
        clean_query = re.sub(r'\s+', ' ', clean_query)
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –∏—â–µ–º –≤–æ –≤—Å–µ—Ö –ø–æ–ª—è—Ö
        if len(clean_query.split()) <= 2:
            return f'all:"{clean_query}"'
        
        # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        return f'ti:"{clean_query}" OR abs:"{clean_query}"'

    def _parse_arxiv_response(self, response_text: str) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ ArXiv API"""
        try:
            papers = []
            root = ET.fromstring(response_text)

            namespaces = ARXIV_NAMESPACES
            
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                title = entry.find('atom:title', namespaces)
                title_text = title.text.strip().replace('\n', ' ')
                
                summary = entry.find('atom:summary', namespaces)
                if summary is not None:
                    summary_text = summary.text.strip().replace('\n', ' ')
                    
                    if len(summary_text) > 200:
                        summary_text = summary_text[:200] + "..."
                else:
                    summary_text = "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                    
                published = entry.find('atom:published', namespaces)
                if published is not None:
                    pub_date = datetime.fromisoformat(published.text.replace('Z', '+00:00'))
                    formatted_date = pub_date.strftime('%Y-%m-%d')
                else:
                    formatted_date = "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                    
                url = entry.find('atom:id', namespaces)
                url_text = url.text.strip() if url is not None else ""
                
                authors = entry.findall('atom:author', namespaces)
                
                author_names = []
                for author in authors:
                    name = author.find('atom:name', namespaces)
                    if name is not None:
                        author_names.append(name.text.strip())
                arxiv_id = self._extract_arxiv_id(url_text)
                
                categories = []
                for category in entry.findall('atom:category', namespaces):
                    term = category.get('term')
                    if term:
                        categories.append(term)
                    
                    paper = {
                        'title': title_text,
                        'authors': author_names,
                        'url': url_text,
                        'published_date': formatted_date,
                        'abstract': summary_text,
                        'categories': categories[:3],
                        'arxiv_id': arxiv_id,
                    }
                papers.append(paper)
            return papers
        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–∞—Ä—Å–∏–Ω–≥–µ XML: {e}")
            return []
        except Exception as e:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return []
        
    async def get_paper_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            if not url or not isinstance(url, str):
                logger.error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL")
                return None
            
            url = url.strip()
            if not url:
                logger.error("–ü—É—Å—Ç–æ–π URL")
                return None
            
            arxiv_id = self._extract_arxiv_id(url)
            if not arxiv_id:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å Arxiv ID –∏–∑ URL: {url}")
                return None

            params = {
                'search_query': f'id:{arxiv_id}',
                'start': 0,
                'max_results': 1
            }
            
            try:
                response = await self.session.get(
                    ARXIV_API_BASE_URL,
                    params=params,
                    timeout=API_TIMEOUT_SECONDS
                )
                response.raise_for_status()
                
            except httpx.TimeoutException:
                logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ ArXiv API –¥–ª—è {arxiv_id}")
                return None
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è {arxiv_id}")
                return None
            except httpx.RequestError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {arxiv_id}: {e}")
                return None
            
            if not response.content:
                logger.error(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç ArXiv API –¥–ª—è {arxiv_id}")
                return None
            
            paper_data = self._parse_arxiv_response(response.text)[0]
            if not paper_data:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –¥–ª—è {arxiv_id}")
                return None

            return paper_data
            
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
            return None
    
    def _extract_arxiv_id(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ArXiv ID –∏–∑ URL"""
        try:
            parsed = urlparse(url)
            
            patterns = [
                r'/abs/(\d{4}\.\d{4,5})(v\d+)?',
                r'/pdf/(\d{4}\.\d{4,5})(v\d+)?\.pdf',
                r'/abs/([a-z-]+/\d{7})(v\d+)?',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, parsed.path)
                if match:
                    return match.group(1)
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ArXiv ID: {e}")
            return None

def format_paper_message(paper: Dict[str, Any], index: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—å–µ –¥–ª—è –≤—ã–≤–æ–¥–∞"""
    title = hbold(f"{index}. {paper['title']}")
    
    authors_text = ', '.join(paper['authors'][:3])
    if len(paper['authors']) > 3:
        authors_text += f" –∏ –µ—â–µ {len(paper['authors']) - 3} –∞–≤—Ç–æ—Ä–∞"
    authors = hitalic(authors_text)

    date = f'–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {paper["published_date"]}' if paper['published_date'] else '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
    categories = ''
    if paper['categories']:
        categories = ', '.join(paper['categories'])
    
    summary = f"üìÑ {paper['abstract']}"
    
    # –°—Å—ã–ª–∫–∞
    url = hlink("üîó –ß–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç—å—é", paper['url'])
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—ë –≤–º–µ—Å—Ç–µ
    parts = [title, authors, date]
    if categories:
        parts.append(categories)
    parts.extend([summary, url])
    return '\n'.join(parts)