services:
  # 1. FastAPI Backend
  api:
    build: .
    
    deploy:
      resources:
        limits:
          memory: 512M  # Максимум, сколько может взять
        reservations:
          memory: 128M # Гарантированный минимум
    
    environment:
      - PYTHONPATH=/usr/local/lib/python3.12/site-packages:/app
      
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 1
    ports:
      - "8000:8000"
    volumes:
      - .:/app  # Монтируем код, чтобы правки применялись без пересборки (Hot Reload)
    restart: always
    healthcheck:  # Добавьте healthcheck для проверки состояния
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 2. Telegram Bot
  bot:
    build: .
    deploy:
      resources:
        limits:
          memory: 256M
    command: python main.py
    volumes:
      - .:/app
    depends_on:
      api:
        condition: service_healthy  
    restart: always
    environment:
      # Если боту нужно знать адрес API внутри сети докера
      - API_URL=http://api:8000 

  # 3. Ollama (локальная LLM)
  ollama:
    image: ollama/ollama:latest
    deploy:
      resources:
        limits:
          memory: 4G  # Для 3B модели достаточно 4GB
        reservations:
          memory: 2G
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama  # Сохраняем модели между перезапусками
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Даём время на загрузку

  # 4. Инициализация модели Ollama (одноразовый запуск)
  ollama-init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      sh -c "curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"qwen2.5:3b\"}' && echo 'Model pulled successfully'"
    restart: "no"

  # 5. Туннель (Ngrok)
  # Используем официальный образ ngrok
#  tunnel:
#    image: ngrok/ngrok:latest
#    command: http api:8000 --host-header=rewrite --url ${WEBAPP_URL} # Туннелируем сервис "api" на порт 8000
#    environment:
#      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN} # Токен берется из .env файла
#      - NGROK_HOST=${NGROK_HOST:-0.0.0.0}
#    ports:
#      - 4040:4040 # Инспектор запросов
#    depends_on:
#      - api

  cloudpub:
    image: cloudpub/cloudpub:latest
    command: run
    restart: unless-stopped
    environment:
      - TOKEN=${CLOUDPUB_TOKEN}
      - HTTP=api:8000
    depends_on:
      - api
    ports:
      - 4040:4040
    volumes:
      - cloudpub-config:/home/cloudpub

volumes:
  ollama_data:  # Persistent storage для моделей Ollama
  cloudpub-config:
